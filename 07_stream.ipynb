{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dornercr/DSCI511/blob/main/07_stream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK7LBLl4Pxmi"
      },
      "source": [
        "# DSCI 511: Data acquisition and pre-processing<br>Chapter 7: Building and Maintaining a Robust Acquisition Stream\n",
        "\n",
        "## 7.0 Callenges with live or recurrent data acquisition\n",
        "We've already discussed some basic methods for acquiring a dataset and also for working with said data,\n",
        "but what if we are working on a project where we'd like to continually collect data? Obviously, creating a data\n",
        "acquisition stream can be very difficult, as there are way more moving pieces at work (so a lot more things can\n",
        "go wrong). For example, here are some of the challenges we might face creating an acquisition stream:\n",
        "* Controlling your rate of access to follow rate limits\n",
        "* Avoiding obtaining redundant data\n",
        "* Intelligent storage of large and/or varietal data\n",
        "* Running your application recurrently with a desired frequency\n",
        "* Handling errors or missing data in a way that won't collapse the stream\n",
        "* Handling potential system failure out of your control (equipment failure)\n",
        "* Dealing with constantly changing APIs and terms of service (very important!)\n",
        "\n",
        "## 7.1 Rate limiting\n",
        "A _rate limit_ refers to a specific rate at which the owner of a source of data allows other internet users to obtain that data. Some sources of data have no rate limit (which is probably a bad idea, potentially allowing for easy DoS attacks), but on most sites you'll encounter they'll have one. Each site sets their own limits, and they often vary wildly from platform to platform (or even on a single platform over time.\n",
        "\n",
        "### 7.1.1 Rate limiting with APIs\n",
        "If you're using an API to collect data there'll almost certainly be a rate limit. When APIs are more commoditized/commercialized their platforms will generally reflect this with more watchfull behaviors that shut your app down quickly. While your app might depend on a particular rate limit, it's not uncommon for these to change, too. For example, the Facebook Graph API used to have a limit of about 3600 calls per hour for any app in development more, but it reduced this to 200 in 2018 with v.3). Generally, it is the duty of the collector to be diligent of rate limits both as a kindness to hosts and to make streams robust. This means following details on rate limiting in the documentation for the data you are working with. For example, current pointers to Twitter and Facebook are here:\n",
        "- Facebook: https://developers.facebook.com/docs/graph-api/advanced/rate-limiting/\n",
        "- Twitter: https://developer.twitter.com/en/docs/basics/rate-limiting.html\n",
        "\n",
        "#### 7.1.1.2 Exercise: Understanding API rate limits\n",
        "Read each of the above API docs and describe the how much API usage is allowed per day from each platform for a given app. Do all apps get the same bandwidth? What methods/metrics do the platforms use to determine limits and overuse? How should an app be constructed to maximize data access?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRzi8YxsPxmj"
      },
      "source": [
        "_Response._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdY1ncJfPxml"
      },
      "source": [
        "#### 7.1.1.3 Monitoring API Rate Limits\n",
        "When it comes to APIs, the actual process of obtaining the data itself is usually\n",
        "orders of magnitude easier than by writing scripts to scrape it, but this comes at the cost of oftentimes stricter and more complicated rate limiting. Also, APIs and their rate limits can change at any moment! So, it's definitely important to check the documentation of the API you're using very frequently, and to become very familiar with it. This is the place (usually) to find the rate limiting information for the API.\n",
        "\n",
        "Oftentimes, there are several components to API rate limiting. For example, on the Facebook Graph API there are three separate categories that you need to look out for (on an hourly basis): total number of calls, total time used, and the total CPU usage. If your script goes over any of these three categories in an hour, then you'll be locked out of further use of the API until the next hour. Unfortunately, this means that if we want to remain within the rate limits, we need to do a lot more work than just setting some sleep call. We might be tempted to just run a few calls, note how much of each category these calls \"use up\", then divide an hour by these figures to get the number of calls we might be allowed per hour. But this isn't a good idea, because every call is different. Some may take up a lot of CPU power, while others take up very little.\n",
        "\n",
        "So, how do we deal with these rate limits? Luckily, most APIs have a method that will, when queried, return to the user various statistics regarding current API usage. A good solution to our problem is to have our script request these statistics after each call, and make sure to only continue making calls while under the rate limits. For a transparent example we'll use the Facebook Graph API.\n",
        "\n",
        "Note: the Facebook example code will not run unless you authenticate with application and access tokens and pass application review and hence has more value for use conceptually. Information on these processes can be found through the (very dense) graph api documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzIL2f3fPxmm"
      },
      "source": [
        "#### 7.1.1.4 Example: Facebook Graph API\n",
        "First, we should pick upper limits for the various rate limit categories so that the script will stop running once they are hit. Upon examining the documentation, we see that the rate limiting data for the three categories are returned as percentages, so once we hit 100% on any of the three categories, the API will be shut off for the rest of the hour. So, to be extra careful, let's tell our script to shut off once any of these categories hits 95%. Next, we need to figure out how to get the rate limit data itself! It turns out that this data is returned with all API calls as an _HTML header_. Basically, this is an extra bit of information (request metadata) that can come along with URL requests. To get the response headers from a urllib response, simply invoke the method: `response.info()`.\n",
        "\n",
        "Upon inspection of the Facebook documentation:\n",
        "- https://developers.facebook.com/docs/graph-api/advanced/rate-limiting/\n",
        "\n",
        "the limiting information is returned as a dictionary object of the following form:\n",
        "```\n",
        "{\n",
        "  \"call_count\"    : x,\n",
        "  \"total_time\"    : y,\n",
        "  \"total_cputime\" : z\n",
        "}\n",
        "```\n",
        "\n",
        "So we just need to get this `dict` and read `x`, `y`, and `z`.\n",
        "Luckily, our old tools for accessing HTML will allow us also to read this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tR47D0GPxmm"
      },
      "outputs": [],
      "source": [
        "## supposing we wanted/were authorized to access the Drexel page's feed\n",
        "## the following URL would return the most recent post\n",
        "url = \"https://graph.facebook.com/drexeluniv/feed?&fields=attachments,created_time,message&limit=1\"\n",
        "\n",
        "## We already decided on capping the rate limit at 95%\n",
        "RATE_MAX = 95\n",
        "\n",
        "## Initialize some variables to store the category data\n",
        "total_time, total_cpu, calls = 0, 0, 0\n",
        "\n",
        "## Now let's use our trusty friend the While loop to keep running\n",
        "## and collect the latest post so long as we don't hit the API max:\n",
        "while total_time < RATE_MAX and total_cpu < RATE_MAX and calls < RATE_MAX:\n",
        "\n",
        "    ## build the request\n",
        "    request = urllib.request.Request(url = url)\n",
        "\n",
        "    ## Open the URL\n",
        "    response = urllib.request.urlopen(request)\n",
        "\n",
        "    ## Now we can grab the rate limit dict using the .info() method\n",
        "    headers = dict(web_response.info())\n",
        "    total_time = headers['total_time']\n",
        "    total_cpu = headers['total_cputime']\n",
        "    calls = headers['call_count']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD6e52DqPxmn"
      },
      "source": [
        "This loop will run until one of the three categories hits 95% usage, and then stop. So now, if we can run this script every hour, we'll be sure to never be rate-limited, and all will be well!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1120am8IPxmn"
      },
      "source": [
        "#### 7.1.1.5 Example: http headers from Twitter using `Twython`\n",
        "Using `urllib` its straightforward to view the headers in a reqponse to a urllib request, using the syntax `response.info()`. Howrever, url construction and authorization are a bit more complicated with Twitter:\n",
        "- https://developer.twitter.com/en/docs/basics/authentication/guides/authorizing-a-request.html\n",
        "\n",
        "so for a working example we'll take the headers as being passed down from a Python API client we're familiar with: `Twython`. According to the rate limiting docs:\n",
        "- https://developer.twitter.com/en/docs/basics/rate-limiting.html\n",
        "\n",
        "the headers we're interested in are:\n",
        "- `x-rate-limit-limit`: the rate limit ceiling for that given endpoint\n",
        "- `x-rate-limit-remaining`: the number of requests left for the 15 minute window\n",
        "- `x-rate-limit-reset`: the remaining window before the rate limit resets, in UTC epoch seconds\n",
        "\n",
        "To get the header's back from `Twython` we can consult the docs:\n",
        "- https://twython.readthedocs.io/en/latest/usage/advanced_usage.html#access-headers-of-previous-call\n",
        "\n",
        "These can be accessed from the previous Twython call using the `.get_lastfunction_header(header)` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aEBwNATbPxmn",
        "outputId": "5133a98e-371f-4cec-c87c-01d90f6ae2b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://twitpic.com/135xa - There's a plane in the Hudson. I'm on the ferry going to pick up the people. Crazy.\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 899\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Helicopter hovering above Abbottabad at 1AM (is a rare event).\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 898\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "So I'm told by a reputable person they have killed Osama Bin Laden. Hot damn.\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 897\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "just setting up my twttr\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 896\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "India has won! à¤­à¤¾à¤°à¤¤ à¤•à¥€ à¤µà¤¿à¤œà¤¯à¥¤ à¤…à¤šà¥à¤›à¥‡ à¤¦à¤¿à¤¨ à¤†à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤¹à¥ˆà¤‚à¥¤\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 895\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "We can neither confirm nor deny that this is our first tweet.\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 894\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Thank you for the @Twitter welcome! We look forward to sharing great #unclassified content with you.\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 893\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "@CIA We look forward to sharing great classified info about you http://t.co/QcdVxJfU4X https://t.co/kcEwpcitHo More: https://t.co/PEeUpPAt7F\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 892\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "If only Bradley's arm was longer. Best photo ever. #oscars http://t.co/C9U5NOtGap\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 891\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Four more years. http://t.co/bAJE6Vom\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 890\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Facebook turned me down. It was a great opportunity to connect with some fantastic people. Looking forward to life's next adventure.\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 889\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Got denied by Twitter HQ. That's ok. Would have been a long commute.\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 888\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Are you ready to celebrate?  Well, get ready: We have ICE!!!!! Yes, ICE, *WATER ICE* on Mars!  w00t!!!  Best day ever!!\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 887\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Hello Twitterverse! We r now LIVE tweeting from the International Space Station -- the 1st live tweet from Space! :) More soon, send your ?s\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 886\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "I'm safely on the surface of Mars. GALE CRATER I AM IN YOU!!! #MSL\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 885\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "@Cmdr_Hadfield Are you tweeting from space? MBB\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 884\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "@WilliamShatner Yes, Standard Orbit, Captain. And we're detecting signs of life on the surface.\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 883\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Everest summit! -Sent with @DeLormeGPS Earthmate PN-60w\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 882\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Arrested\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 881\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Just another night of playing Cards Against Humanity... http://t.co/lfu3YtdHRC\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 880\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "Ugh - NEVER going to a Ryan Gosling movie in a theater again. Apparently masturbating in the back row is still considered \"inappropriate\"\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 879\n",
            "x-rate-limit-reset 1634513755\n",
            "\n",
            "I don't know why I wasn't invited, I'm great at weddings... @KimKardashian @kanyewest\n",
            "x-rate-limit-limit 900\n",
            "x-rate-limit-remaining 878\n",
            "x-rate-limit-reset 1634513755\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from twython import Twython\n",
        "\n",
        "## place authorization strings here to run code\n",
        "consumer_key = \"\"\n",
        "consumer_secret = \"\"\n",
        "access_token = \"\"\n",
        "access_token_secret = \"\"\n",
        "\n",
        "## initilize the module\n",
        "twitter = Twython(consumer_key, consumer_secret)\n",
        "\n",
        "## The notable tweet IDs from Chapter 3\n",
        "IDs = [\"1121915133\", \"64780730286358528\", \"64877790624886784\", \"20\", \"467192528878329856\",\n",
        "       \"474971393852182528\", \"475071400466972672\", \"475121451511844864\", \"440322224407314432\",\n",
        "       \"266031293945503744\", \"3109544383\", \"1895942068\", \"839088619\", \"8062317551\", \"232348380431544320\",\n",
        "       \"286910551899127808\", \"286948264236945408\", \"27418932143\", \"786571964\",\n",
        "       \"467896522714017792\", \"290892494152028160\", \"470571408896962560\"]\n",
        "\n",
        "headers = ['x-rate-limit-limit', 'x-rate-limit-remaining', 'x-rate-limit-reset']\n",
        "\n",
        "for ID in IDs:\n",
        "    status = twitter.show_status(id = ID)\n",
        "    print(status[\"text\"])\n",
        "    for header in headers:\n",
        "        print(header, twitter.get_lastfunction_header(header))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ajw9OHLPxmo"
      },
      "source": [
        "### 7.1.2 Rate limited web content: Robots.txt\n",
        "Twitter and Facebook both have extensive documentation regarding using their APIs and the rate limits inherent to both.\n",
        "But what if you've scoured your data source's website and haven't been able to find any information\n",
        "regarding their rate limiting? Well, first make sure you've checked the terms of service. Oftentimes, you can find\n",
        "your legal rights in both using and harvesting the data. Apart from this, something almost every website has is a\n",
        "text file calls `robots.txt`. This file is used to tell web crawling programs (usually referred to as __spiders__) how\n",
        "they should behave, and also even bans certain programs from accessing the data at all! Programs which are known to be\n",
        "abusive and which refuse to follow rate-limiting guidelines can end up being banned. How are they set up? Well, they're actually written in a way that's pretty close to plain English. Let's look at Twitter's robots.txt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iz-PMSo6Pxmp",
        "outputId": "44fc4fb0-f402-4e4c-b3bf-6cb776a425c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Google Search Engine Robot\n",
            "# ==========================\n",
            "User-agent: Googlebot\n",
            "\n",
            "Allow: /*?lang=\n",
            "Allow: /hashtag/*?src=\n",
            "Allow: /search?q=%23\n",
            "Allow: /i/api/\n",
            "Disallow: /search/realtime\n",
            "Disallow: /search/users\n",
            "Disallow: /search/*/grid\n",
            "\n",
            "Disallow: /*?\n",
            "Disallow: /*/followers\n",
            "Disallow: /*/following\n",
            "\n",
            "Disallow: /account/deactivated\n",
            "Disallow: /settings/deactivated\n",
            "\n",
            "Disallow: /[_0-9a-zA-Z]+/status/[0-9]+/likes\n",
            "Disallow: /[_0-9a-zA-Z]+/status/[0-9]+/retweets\n",
            "Disallow: /[_0-9a-zA-Z]+/likes\n",
            "Disallow: /[_0-9a-zA-Z]+/media \n",
            "Disallow: /[_0-9a-zA-Z]+/photo\n",
            "\n",
            "# Every bot that might possibly read and respect this file\n",
            "# ========================================================\n",
            "User-agent: *\n",
            "Disallow: /\n",
            "\n",
            "# WHAT-4882 - Block indexing of links in notification emails. This applies to all bots.\n",
            "# =====================================================================================\n",
            "Disallow: /i/u\n",
            "Noindex: /i/u\n",
            "\n",
            "# Wait 1 second between successive requests. See ONBOARD-2698 for details.\n",
            "Crawl-delay: 1\n",
            "\n",
            "# Independent of user agent. Links in the sitemap are full URLs using https:// and need to match\n",
            "# the protocol of the sitemap.\n",
            "Sitemap: https://twitter.com/sitemap.xml\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = \"https://twitter.com/robots.txt\"\n",
        "\n",
        "# Make the request\n",
        "req = urllib.request.Request(url = url)\n",
        "\n",
        "# Open the URL\n",
        "handler = urllib.request.urlopen(req)\n",
        "\n",
        "# Read/view the data as a string\n",
        "robots = handler.read().decode('utf-8')\n",
        "print(robots)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUuDbzZDPxmp"
      },
      "source": [
        "#### 7.1.2.1 What does this all mean?\n",
        "Near the bottom we see a line that says `Crawl-delay: 1`. This is the rate limit! This is telling spiders to wait 1 second in between calls, so if we decided to scrape Twitter, we'd have to make sure to do it no more than 3600 times per hour. But what is all the other stuff?\n",
        "\n",
        "When you access web content, you always send some information to the server about your own identity (at least, roughly where you are and what browser you're using). This information is referred to as your __`User-agent`__. The file above forbids certain User-agents from crawling specific portions of Twitter with the `Disallow` tag. Notice as well that there's a somewhat mysterious comment above the wild card (`*`) `User-agent`:\n",
        "```\n",
        "# Every bot that might possibly read and respect this file.\n",
        "User-agent: *\n",
        "Allow: /*?lang=\n",
        "Allow: /hashtag/*?src=\n",
        "Allow: /search?q=%23\n",
        "Disallow: /search/realtime\n",
        "Disallow: /search/users\n",
        "Disallow: /search/*/grid\n",
        "\n",
        "Disallow: /*?\n",
        "Disallow: /*/followers\n",
        "Disallow: /*/following\n",
        "```\n",
        "Technically, this pertains to us or any bot we create!\n",
        "#### 7.1.2.3 Exercise: robots.txt\n",
        "Take a look at the robots file for each of `facebook.com` and `amazon.com`. Determine and discuss any allowances/disallowances for bots that you might create to crawl these sites. Do you infer any cultural differences around data sharing and access between these companys and also with Twitter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WC13gT2Pxmq"
      },
      "source": [
        "_Response._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSMPOGtPPxmq"
      },
      "source": [
        "#### 7.1.2.4 implementing the rules in robots.txt\n",
        "So, how can we follow these rules that are handed to us by websites? Well, as burgeoning data mungers we might be tempted to attach `robots.txt` as a regex challenge. As with most things in Python there's another handy module. There's one inside of urllib: `robotparser`, but instead we'll be using an improved module built around the rules that the big tech companies like Google use. There's a lot of history and disagreement with no truly universal standard on how to parse/interpret robots.txt, but the makers of the improved module, `robotexclusionrulesparser`, provide a nice discussion of the ecosystem:\n",
        "- https://github.com/osvenskan/rerp\n",
        "\n",
        "Let's use `robotexclusionrulesparser.RobotFileParserLookalike` to confirm that we're allowed to search for the top (`/search?q=%23`) and not the most recent (`/search/realtime`) tweets matching a search term; we'll use `'data science'`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgl3Tl8CPxmq",
        "outputId": "a2b47a34-10fe-4c69-f3ea-7af1479340b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "## don't use this one if you want to actually scrape\n",
        "## it will basically just tell you that you can't scape things\n",
        "# import urllib.robotparser as robotparser\n",
        "\n",
        "## use this updated module to access the big tech parse for scraping rules\n",
        "import robotexclusionrulesparser\n",
        "\n",
        "## spin up the module\n",
        "rp = robotexclusionrulesparser.RobotFileParserLookalike()\n",
        "\n",
        "## parse the robots file\n",
        "rp.parse(robots)\n",
        "\n",
        "## we're not allowed to scrape the top, relatively current matches\n",
        "print(rp.can_fetch(\"*\", \"https://twitter.com/search?q=%23/data science\"))\n",
        "## we're not entitled to scrape the most current matches\n",
        "print(rp.can_fetch(\"*\", \"https://www.twitter.com/search/realtime/data science\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKNiV6fcPxmq"
      },
      "source": [
        "#### 7.1.2.5 Specifying a `User-agent`\n",
        "Since we'll be accessing web content using Python and not a browser, we have the option of setting our `User-agent`. This can allow us to get more data than if we were to just leave it unspecified. If you're trying to access data online and you're getting errors, a handy trick to fix it is specify a `User-agent` that would ordinarily be presented by a web browser, like `'Mozilla/5.0'`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zheGP9NPPxmr",
        "outputId": "6b658e05-d457-48b5-a813-490c3f093da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html>\n",
            "\n",
            "<html>\n",
            "<head>\n",
            "<title>Example Domain</title>\n",
            "<meta charset=\"utf-8\"/>\n",
            "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
            "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
            "<style type=\"text/css\">\n",
            "    body {\n",
            "        background-color: #f0f0f2;\n",
            "        margin: 0;\n",
            "        padding: 0;\n",
            "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
            "        \n",
            "    }\n",
            "    div {\n",
            "        width: 600px;\n",
            "        margin: 5em auto;\n",
            "        padding: 2em;\n",
            "        background-color: #fdfdff;\n",
            "        border-radius: 0.5em;\n",
            "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
            "    }\n",
            "    a:link, a:visited {\n",
            "        color: #38488f;\n",
            "        text-decoration: none;\n",
            "    }\n",
            "    @media (max-width: 700px) {\n",
            "        div {\n",
            "            margin: 0 auto;\n",
            "            width: auto;\n",
            "        }\n",
            "    }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
            "<div>\n",
            "<h1>Example Domain</h1>\n",
            "<p>This domain is for use in illustrative examples in documents. You may use this\n",
            "    domain in literature without prior coordination or asking for permission.</p>\n",
            "<p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
            "</div>\n",
            "</body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's say we're having trouble opening up our trusty example page, example.com\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "url = 'http://www.example.com/'\n",
        "\n",
        "# Create the user-agent header\n",
        "# NOTE: This is about as basic as they come. This just tells the site that you're using Mozilla Firefox,\n",
        "# which makes you look more like a human than just using the command line to make requests.\n",
        "header = {'User-Agent': 'Mozilla/5.0'}\n",
        "\n",
        "# Make the request\n",
        "req = urllib.request.Request(url = url, headers = header)\n",
        "\n",
        "# Open the URL\n",
        "handler = urllib.request.urlopen(req)\n",
        "\n",
        "soup = BeautifulSoup(handler.read(), 'html.parser')\n",
        "print(soup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-ESBSSzPxmr"
      },
      "source": [
        "There are all kinds of possibilities for a User-Agent, but the vast majority of the time, if changing the User-Agent is going to help you get better results, just using this basic configuration supplied will do the trick.\n",
        "\n",
        "### 7.1.3 Sleep timers\n",
        "After making sure that we are allowed to obtain data from a website and determining what the time limitations are for acquiring said data, and possibly having set up the appropriate `User-Agent`, we might actually be getting data! So how do we make sure to follow the time limitations? By telling Python to sleep!\n",
        "\n",
        "The easiest way to make sure we don't overload a website is by telling Python to wait a certain amount of time after making each request. For example, it's common for `robots.txt` to tell spiders to wait 30 seconds in between web scrapes. So here we'd just make sure to tell Python to wait 30 seconds each time after we use `urllib.request`. This functionality occurs in the `time` module. So, let's say we want to (for some reason) scrape and print out the HTML of `http://example.com/` every 30 seconds, forever:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "frgCuim_Pxms",
        "outputId": "d53056a7-cbc7-4c06-dc53-2ba2ea011be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finished request number 1\n",
            "finished request number 2\n",
            "finished request number 3\n",
            "finished request number 4\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib\n",
        "\n",
        "counter = 0\n",
        "while True: # The easiest way to set up an infinite loop in Python\n",
        "    # This is exactly the same code as before\n",
        "    counter += 1 # Don't want this to run forever!\n",
        "    html_text = urllib.request.urlopen(\"http://www.example.com/\").read()\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    print(\"finished request number\", counter)\n",
        "    time.sleep(30) # This command makes Python do nothing for 30 seconds\n",
        "    if counter > 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mX1c-lBPxms"
      },
      "source": [
        "You don't have to put a set number inside `time.sleep()`. If we wanted to, we could create a script that has a variable wait time instead, perhaps dynamically in response to a platforms rate limit information. As a simple example, after each successive call let's make the script wait an additional second:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "S_4JoCUxPxms",
        "outputId": "9efb2d98-4e65-49e7-82f0-e41239b8126b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finished request number 1 after waiting for  1 seconds\n",
            "finished request number 2 after waiting for  2 seconds\n",
            "finished request number 3 after waiting for  3 seconds\n",
            "finished request number 4 after waiting for  4 seconds\n"
          ]
        }
      ],
      "source": [
        "wait_time = 0\n",
        "counter = 0\n",
        "\n",
        "while True:\n",
        "    counter += 1\n",
        "    wait_time += 1  # Increment the wait time\n",
        "    html_text = urllib.request.urlopen(\"http://www.example.com/\").read()\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    print(\"finished request number\", counter, \"after waiting for \", wait_time, \"seconds\")\n",
        "    time.sleep(wait_time)\n",
        "    if counter > 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw_hPnODPxms"
      },
      "source": [
        "When it comes to web scraping, it helps to appear less robotic and more human in our scraping. In this case, we could make the amount of time that we wait in between scrapes random:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "rZHcKQyLPxmt",
        "outputId": "23656eff-0f08-4161-bdf3-1d17013beb6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finished request number 1 after waiting for 40 seconds\n",
            "finished request number 2 after waiting for 44 seconds\n",
            "finished request number 3 after waiting for 51 seconds\n",
            "finished request number 4 after waiting for 34 seconds\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "counter = 0\n",
        "\n",
        "while True:\n",
        "    counter += 1\n",
        "    wait_time = 30 + random.randrange(0, 30)  # Here the wait time is a random number between 30 and 60\n",
        "    html_text = urllib.request.urlopen(\"http://www.example.com/\").read()\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    print(\"finished request number\", counter, \"after waiting for\", wait_time, \"seconds\")\n",
        "    time.sleep(wait_time)\n",
        "    if counter > 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIIsZv8fPxmt"
      },
      "source": [
        "## 7.2 Recurrent script execution\n",
        "\n",
        "### 7.2.1 The cron utility\n",
        "\n",
        "Alright, so let's assume that we're working with an API that has hourly rate limits (most of them), and we're finished writing our script which will acquire data from the API while respecting the rate limits. How do we get our computer to automatically run this script hourly? If you're running a UNIX-like operating system (Mac OS X, any flavor of GNU/Linux, or BSD), then this functionality is built into the `cron` utility.\n",
        "\n",
        "`cron` is a time-based job scheduling utility for UNIX-like systems used to schedule jobs to run periodically at fixed intervals of time. Basically, `cron` uses a cron table file (called `crontab`) to execute commands at the specified times. To open up the `crontab` file for editing, just type the following into your terminal:\n",
        "\n",
        "`$ crontab -e`\n",
        "\n",
        "### 7.2.1.1 Editing crontab\n",
        "\n",
        "This will bring up a text file in your default text editor. So, first we need to see where the script that we'd like to run hourly is located on the machine. You need to specify the whole path, so if you aren't sure, navigate to the file(s) in your terminal, then use the UNIX command `pwd` (present working directory), which will display the full path to the current directory you are in. So, as an example, let's say we've done this, and we'd like to run a file named `script.py` hourly with the following path: `/Projects/scraper/script.py`.\n",
        "\n",
        "Once we're editing the `crontab`, we can add a new job by adding the correct line of text. This line will fit the following schema:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aJj4siwPxmt"
      },
      "source": [
        "```\n",
        "# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\n",
        "# â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hour (0 - 23)\n",
        "# â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of month (1 - 31)\n",
        "# â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ month (1 - 12)\n",
        "# â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of week (0 - 6) (Sunday to Saturday;\n",
        "# â”‚ â”‚ â”‚ â”‚ â”‚                                       7 is also Sunday on some systems)\n",
        "# â”‚ â”‚ â”‚ â”‚ â”‚\n",
        "# â”‚ â”‚ â”‚ â”‚ â”‚\n",
        "# * * * * *  command to execute\n",
        "```\n",
        "\n",
        "So, by specifying numbers in the place of the asterisks, one can have jobs run with very specific timings (leaving an asterisk in a spot will ignore that specification, meaning for example an asterisk in the day of the month column will have the command execute every single day of the month).\n",
        "\n",
        "This may seem a bit complicated, and it is. Luckily, most implementations of `cron` allow for shortcuts for widely-used timings. Instead of what's presented above, one can just use:\n",
        "+ `@hourly command to execute` - run the command once an hour\n",
        "+ `@daily command to execute` - run the command once daily\n",
        "+ `@weekly command to execute` - run the command weekly\n",
        "+ etc.\n",
        "\n",
        "So, since we need to call python to execute our script, our command will look like this:\n",
        "\n",
        "`python3 /Projects/scraper/script.py`\n",
        "\n",
        "So, to perform our scraping once hourly, we just add the following line to `crontab`:\n",
        "\n",
        "`@hourly python3 /Projects/scraper/script.py`\n",
        "\n",
        "Make sure to save the text file after adding this, and we're done! Our script will run at the beginning of each hour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYQVLWPsPxmt"
      },
      "source": [
        "#### 7.2.1.2 Exercise: understanding a crontab for a recurrent, whole-site data access application\n",
        "Gutenberg is an open data repository, so we should be able to download all of its data!  To start, let's review the robots file on Project Gutenberg's website:\n",
        "- http://www.gutenberg.org/robots.txt\n",
        "\n",
        "What do you notice about this file. Is anyone allowed to crawl the site? Do you think Gutenberg uses the newer, big tech rules? How frequently can we make requests?\n",
        "\n",
        "Use the `robotexclusionrulesparser` module from Section 7.1.2.4 to determine if we can access a given data file. Use the URL for the text copy of Moby dick:\n",
        "- https://www.gutenberg.org/files/2701/2701-0.txt\n",
        "\n",
        "Following the above, review the instructions on mirroring the repository:\n",
        "- https://www.gutenberg.org/help/mirroring.html\n",
        "\n",
        "and explain why Gutenberg requests using the `rsync` command-line utility to copy its data. Can you decode the two presented crontab patterns?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§© DSCI 511 â€” Chapter 7: Building and Maintaining a Robust Acquisition Stream  \n",
        "\n",
        "## Section 7.2.1.2 â€“ Exercise: Understanding a Crontab for a Recurrent Whole-Site Data Access Application  \n",
        "\n",
        "### *(Project Gutenberg Example)*  \n",
        "\n",
        "Gutenberg is an open data repository, so we should be able to download all of its data!  \n",
        "\n",
        "To start, let's review the robots file on Project Gutenberg's website:  \n",
        "ðŸ‘‰ [http://www.gutenberg.org/robots.txt](http://www.gutenberg.org/robots.txt)  \n",
        "\n",
        "**Questions to consider:**  \n",
        "- What do you notice about this file?  \n",
        "- Is anyone allowed to crawl the site?  \n",
        "- Do you think Gutenberg uses the newer, big-tech rules?  \n",
        "- How frequently can we make requests?  \n",
        "\n",
        "Use the **`robotexclusionrulesparser`** module from *Section 7.1.2.4* to determine if we can access a given data file.  \n",
        "Use the URL for the text copy of *Moby Dick*:  \n",
        "ðŸ‘‰ [https://www.gutenberg.org/files/2701/2701-0.txt](https://www.gutenberg.org/files/2701/2701-0.txt)  \n",
        "\n",
        "Following the above, review the instructions on mirroring the repository:  \n",
        "ðŸ‘‰ [https://www.gutenberg.org/help/mirroring.html](https://www.gutenberg.org/help/mirroring.html)  \n",
        "\n",
        "and explain **why Gutenberg requests using the `rsync` command-line utility** to copy its data.  \n",
        "\n",
        "Finally, **decode the two presented crontab patterns** and explain their meaning in terms of timing and frequency.  \n"
      ],
      "metadata": {
        "id": "B1cN2xk8Xi1_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn9kNkXmPxmu"
      },
      "source": [
        "_Response._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGl2tm2YPxmu",
        "outputId": "cb75518e-eb9f-4205-c034-505013e4b217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can 'DataAcquisitionBot' access https://www.gutenberg.org/files/2701/2701-0.txt? â†’ True\n",
            "Crawl delay for DataAcquisitionBot: Not specified\n",
            "\n",
            "Preview of robots.txt:\n",
            "01: User-agent: *\n",
            "02: Disallow: /ebooks/search\n",
            "03: \n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# DSCI 511 â€” Chapter 7: Building and Maintaining a Robust Acquisition Stream\n",
        "# Section 7.2.1.2 â€“ Exercise: Understanding a Crontab for a Recurrent Whole-Site Data Access Application\n",
        "# Project Gutenberg Example â€” robots.txt + access test (Python 3.12 Compatible)\n",
        "# ============================================================\n",
        "\n",
        "import urllib.robotparser\n",
        "import urllib.request\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 1: Target URLs\n",
        "# ------------------------------------------------------------\n",
        "robots_url = \"https://www.gutenberg.org/robots.txt\"\n",
        "test_url   = \"https://www.gutenberg.org/files/2701/2701-0.txt\"\n",
        "user_agent = \"DataAcquisitionBot\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 2: Initialize and load the robots.txt parser\n",
        "# ------------------------------------------------------------\n",
        "rp = urllib.robotparser.RobotFileParser()\n",
        "rp.set_url(robots_url)\n",
        "rp.read()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 3: Check permissions for the test URL\n",
        "# ------------------------------------------------------------\n",
        "can_fetch = rp.can_fetch(user_agent, test_url)\n",
        "\n",
        "print(f\"Can '{user_agent}' access {test_url}? â†’ {can_fetch}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 4: Display crawl delay (if specified)\n",
        "# ------------------------------------------------------------\n",
        "delay = rp.crawl_delay(user_agent)\n",
        "print(f\"Crawl delay for {user_agent}: {delay if delay is not None else 'Not specified'}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 5: Optional â€“ fetch and print first few lines of robots.txt\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\nPreview of robots.txt:\")\n",
        "with urllib.request.urlopen(robots_url) as response:\n",
        "    for i, line in enumerate(response.read().decode(\"utf-8\").splitlines()[:10]):\n",
        "        print(f\"{i+1:02d}: {line}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM2Wc6dxPxmu"
      },
      "source": [
        "## 7.3 Monitoring processes\n",
        "\n",
        "So, we've set up a stream that follows all the rate limits and automatically executes on an hourly basis, downloading all kinds of useful data. What happens if something goes wrong? As we have it set up, if there's some kind of bug or error which stops the execution of the script, it will go unnoticed until someone decides to manually check up on it. This could mean you haven't been collecting data for days, weeks, or even months! It would be nice, and give us peace of mind, to create an additional mechanism solely dedicated to checking up on our stream, and in case it goes down for some reason, restarting it. This is made easy with the `os` Python module, and a feature on UNIX-like systems called `top`.\n",
        "\n",
        "### 7.3.1 Checking processes with `ps`\n",
        "If you've run Microsoft Windows for any amount of time, you've probably had an application that you were using freeze up. Usually, a simple solution to fix this problem is to just go into the process manager (by pressing `ctrl + alt + del`), and end the process, then restart it back up. This is exactly what we'd like to do with our stream script, but we need to find a way to automate it. Fortunately, there are multiple process managers in UNIX-like systems akin to the Windows process manager. For this example, we'll just use one of the most basic ones available: `ps` (many people use `htop` or`top` but ps has simpler output). The output of `ps` will yield a line for each process currently running.\n",
        "\n",
        "#### 7.3.1.1 A restarter script using `ps`\n",
        "Using the output of `ps`, we can regularly check if our streaming process is running, and if the script finds that it isn't, restart it. This is tricky, involving regular expressions&mdash;we just need to parse the output of the `ps` command, and search it for our specific streaming process.\n",
        "\n",
        "Note: this example is intended to generalize process monitoring to environments where all that is available is text output on active/inactive processes, like on a cluster's queue. For Pythonic use cases, we'll explore a version of this code that uses a Python module (`psutil`), resuing some peices that we first build up now incrementally. The most important thing is to develop ideas for applications in your own work!\n",
        "\n",
        "Before we get started, let's just make a dummy script just loops endlessly, simply sleeping for a second at a time inside of a while loop, randomly exiting with 1/10 probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDEoSTrDPxmu",
        "outputId": "6aa1c9cf-b26f-45fa-e7f1-c022aceaf5b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting dummy.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile dummy.py\n",
        "import random, time\n",
        "\n",
        "while random.random() <= 0.90:\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vps9OpbbPxmv"
      },
      "source": [
        "To run this script we can use the `nohup` command which disconnects the executing terminal from the connection. The `&` at the end specifices that this is a child process, run separately from our python notebook. This means our script will run are our notebook won't have to wait for it to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0ElXnWxPxmv",
        "outputId": "5f580df2-5eba-4b6a-e77c-c0b11b54b26a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "status = os.system(\"nohup python3 ./dummy.py &\")\n",
        "print(status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwWOvyTDPxmv"
      },
      "source": [
        "Now, to get things started we import the other necessary modules, and obtain the text output of the `top` command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "FynOCroHPxmv",
        "outputId": "8f3e5caf-8bfd-42a0-b6f5-8c6e5f9c7835"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['  PID TTY           TIME CMD',\n",
              " '    1 ??         8:51.08 /sbin/launchd',\n",
              " '  106 ??         2:42.57 /usr/libexec/logd',\n",
              " '  107 ??         0:00.30 /usr/libexec/smd',\n",
              " '  108 ??         0:23.31 /usr/libexec/UserEventAgent (System)',\n",
              " '  111 ??         0:02.77 /System/Library/PrivateFrameworks/Uninstall.framework/Resources/uninstalld',\n",
              " '  112 ??         2:22.01 /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/Support/fseventsd',\n",
              " '  113 ??         0:14.07 /System/Library/PrivateFrameworks/MediaRemote.framework/Support/mediaremoted',\n",
              " '  114 ??         0:32.83 /usr/sbin/systemstats --daemon',\n",
              " '  116 ??         0:29.87 /usr/libexec/configd',\n",
              " '  117 ??         0:00.51 endpointsecurityd',\n",
              " '  118 ??         1:29.29 /System/Library/CoreServices/powerd.bundle/powerd',\n",
              " '  119 ??         0:03.20 /System/Library/PrivateFrameworks/BiomeStreams.framework/Support/biomed',\n",
              " '  121 ??         0:04.64 /usr/libexec/amfid',\n",
              " '  123 ??         0:14.45 /usr/libexec/remoted']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import datetime\n",
        "\n",
        "## the os.popen command creates an instance of the process we're opening\n",
        "## that's ready to read into text (this is piping)\n",
        "processes = os.popen(\"ps -A\").read().split(\"\\n\")\n",
        "#processes = os.popen(\"tasklist\").read().split(\"\\n\")# windows computer\n",
        "processes[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB_h09uhPxmw"
      },
      "source": [
        "How do we process this? Since we're going so low level we'll have to use regex. Inspecting the above, the rows appear to have 4 columns, with the last being the process name. Each column is separated by one or more whitespace characters `pattern = '\\s+'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "WOnfH8F6Pxmw"
      },
      "outputs": [],
      "source": [
        "def check_process(process_name):\n",
        "    ## review the current running processes\n",
        "    processes = os.popen(\"ps -A\").read().split(\"\\n\")\n",
        "\n",
        "    ## Get the process names\n",
        "    ## Splits up the row by whitespace, then looks at the last element (the name)\n",
        "    process_names = [re.split('\\s+', row.strip())[-1] for row in processes]\n",
        "\n",
        "    ## create a boolean valued list indicating if the process is running\n",
        "    is_running = [name for name in process_names if re.search(process_name, name)]\n",
        "\n",
        "    ## Let the user know which processes are running\n",
        "    return(is_running)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpWW6gdlPxmw",
        "outputId": "d3b79ca5-17f3-4a4b-e282-222f0cd28af6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['./dummy.py']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# name of the process we're looking for\n",
        "name = \"dummy.py\"\n",
        "check_process(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMsP80qEPxmw"
      },
      "source": [
        "Ok, now we have an idea if our stream is up or not. Next, we need to reset the stream if it isn't running (and keep a note in the log and error files, or do nothing (and keep a note in the log file):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "nvRnVXnAPxmw"
      },
      "outputs": [],
      "source": [
        "## create a log file for our script-monitoring code\n",
        "logfile = 'restarter.log'\n",
        "open(logfile, 'w').close()\n",
        "\n",
        "## perform the initial execution\n",
        "current_time = datetime.datetime.strftime(datetime.datetime.now(), \"%Y-%m-%d-%H-%M\")\n",
        "status = os.system(\"nohup python3 ./dummy.py &\")\n",
        "with open(logfile, 'a') as f:  # Open the logfile\n",
        "    f.writelines(\"Started the process \" + name +\"\\n\")\n",
        "\n",
        "while 1:\n",
        "    current_time = datetime.datetime.strftime(datetime.datetime.now(), \"%Y-%m-%d-%H-%M\")\n",
        "    instances = check_process(name)\n",
        "\n",
        "    if instances:  # The stream is working fine\n",
        "        with open(logfile, 'a') as f:  # Open the logfile\n",
        "            f.writelines(current_time + \" : The process \" + name + \" is running \\n\")  # Write that all is well\n",
        "            f.writelines(\"sleeping for another 10 seconds...\\n\\n\")\n",
        "    else:  # Stream is down\n",
        "        with open(logfile, 'a') as f:  # Open the logfile\n",
        "            f.writelines(current_time + \" : The process \" + name + \" died! Restarting... \\n\") # Note error\n",
        "        status = os.system(\"nohup python3 ./dummy.py &\")  # Finally, restart the stream\n",
        "        ## for the notebook---break if the code got restarted\n",
        "        break\n",
        "    ## sleep for another 2 seconds\n",
        "    time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqWrifqjPxmx",
        "outputId": "0dadfa4c-7be3-4452-c91a-3dafe24d4225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started the process dummy.py\r\n",
            "2023-09-18-21-27 : The process dummy.py is running \r\n",
            "sleeping for another 10 seconds...\r\n",
            "\r\n",
            "2023-09-18-21-27 : The process dummy.py died! Restarting... \r\n"
          ]
        }
      ],
      "source": [
        "!cat restarter.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxHO4MbsPxmx"
      },
      "source": [
        "### 7.3.2 Monitoring processes with `psutil`\n",
        "Knowing how to regex with command-line utilities like `ps`, `top`, `htop`, or anything text based on a cluster or server is important to have in your back pocket, but in a basic local environment there's of course a more Pythonic way. There's a Python module specifically geared towards working with your machine's currently running processes&mdash;`psutil`.  While the regex approach with `ps` is one that we can generalize to other process-monitoring scenarios, we can use `psutil` to much more easily get a list of our processes on a single machine. Here's here's how we can get the process names using `psutil`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AJsuTnkPxmx",
        "outputId": "972e0f1a-559f-4039-b49e-d1544d6c9b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kernel_task running 0\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "# use psutil to easily get the list of processes we want\n",
        "processes = list(psutil.process_iter())\n",
        "\n",
        "# we can use psutil to easily access all sorts of information\n",
        "print(processes[0].name(), processes[0].status(), processes[0].pid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKSSUZWmPxmx"
      },
      "source": [
        "Note: psutil is a little tricky and the `.name()` method throws an error if a process is has `status='zombie'`. Apparently, using `ps` in our `os.system()` command has left it a zombie! Let's gather the names of the zombie and non-zombie processes using a list comprehension with some control for status:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LvbGNLcPxmx",
        "outputId": "64de9eac-5371-476a-87fa-8b609765a18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the zombie processes: \n",
            "[psutil.Process(pid=44420, name='ps', status='zombie', started='21:27:55')]\n",
            "\n",
            "Here are the living processes: \n",
            "['kernel_task', 'launchd', 'logd', 'smd', 'UserEventAgent', 'uninstalld', 'fseventsd', 'mediaremoted', 'systemstats', 'configd']\n"
          ]
        }
      ],
      "source": [
        "processes = list(psutil.process_iter())\n",
        "\n",
        "## collect the process names as long as they're not zombies\n",
        "process_names = [process.name() for process in processes if process.status() != \"zombie\"]\n",
        "## collect the zombies\n",
        "zombies = [process for process in processes if process.status() == \"zombie\"]\n",
        "\n",
        "print(\"Here are the zombie processes: \")\n",
        "print(zombies)\n",
        "print()\n",
        "\n",
        "print(\"Here are the living processes: \")\n",
        "print(process_names[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET7zbCcfPxmy"
      },
      "source": [
        "#### 7.3.2.1 Killing a process\n",
        "What do we do with a process running in the background that we need to stop? Well, it's not actally possible to kill zombies&mdash;those are unreaped child processes that will disappear once their parent processes finish (e.g., the server running this notebook). However, if we need to take down our streaming application for service or anything else we might need to interrupt it. This\n",
        "can be done using the `kill <PID>` bash utility, after obtaining the `PID` information from psutil as `process.pid`, or from reading/parsing one of the command-line utilities. However, `psutil` makes it even easier with the `process.kill()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bDAsvtmPxmy"
      },
      "source": [
        "#### 7.3.2.2 Exercise: a script restarter using psutil that also kills zombies\n",
        "Rewrite `check_process(name)` above by using psutil to 1) obtain process names more easily without regex, and use this 2) to restart our dummy process if it's finished after 3 or fewer passes in the while loop, and kill it if it's still running after 4 or more passes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl2Hzyf3Pxmy"
      },
      "outputs": [],
      "source": [
        "## Enter code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}